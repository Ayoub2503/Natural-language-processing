{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuVouapRmjEW"
   },
   "source": [
    "<center><h2>ALTeGraD 2021<br>Lab Session 1: HAN</h2><h3>Hierarchical Attention Network Using GRU</h3> 09 / 11 / 2021<br> M. Kamal Eddine, H. Abdine</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogg6hkhFW8Jz",
    "outputId": "989c8990-1f76-4ee6-d3fc-6bfaa10507a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-16 19:21:53--  https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199289&authkey=AHgxt3xmgG0Fu5A\n",
      "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
      "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://vqtlqw.am.files.1drv.com/y4m2Gi6W5415zprJvl85t6qr1RR2YnYvW1j0U4CDwImFbPODca-Pn-fhFNfg10A40vdZ8u95yJkINsHyWli28w4U_O_8HV5D0rGYXwIMq0Z1Rw2XcAM9x2atH-4Oe2SMP1Q_2EVvBBGHIUyFc3P9X4NbbZVulLo9DrMRsGjc6j_sFpCj-2hgT_PDYV4Qd-Zp7ki0SaiQDaX2cacdmpmomEYGw/data.zip?download&psid=1 [following]\n",
      "--2021-11-16 19:21:54--  https://vqtlqw.am.files.1drv.com/y4m2Gi6W5415zprJvl85t6qr1RR2YnYvW1j0U4CDwImFbPODca-Pn-fhFNfg10A40vdZ8u95yJkINsHyWli28w4U_O_8HV5D0rGYXwIMq0Z1Rw2XcAM9x2atH-4Oe2SMP1Q_2EVvBBGHIUyFc3P9X4NbbZVulLo9DrMRsGjc6j_sFpCj-2hgT_PDYV4Qd-Zp7ki0SaiQDaX2cacdmpmomEYGw/data.zip?download&psid=1\n",
      "Resolving vqtlqw.am.files.1drv.com (vqtlqw.am.files.1drv.com)... 13.107.43.12\n",
      "Connecting to vqtlqw.am.files.1drv.com (vqtlqw.am.files.1drv.com)|13.107.43.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12650196 (12M) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]  12.06M  8.53MB/s    in 1.4s    \n",
      "\n",
      "2021-11-16 19:21:56 (8.53 MB/s) - ‘data.zip’ saved [12650196/12650196]\n",
      "\n",
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "  inflating: __MACOSX/._data         \n",
      "  inflating: data/labels_train.npy   \n",
      "  inflating: __MACOSX/data/._labels_train.npy  \n",
      "  inflating: data/docs_test.npy      \n",
      "  inflating: __MACOSX/data/._docs_test.npy  \n",
      "  inflating: data/labels_test.npy    \n",
      "  inflating: __MACOSX/data/._labels_test.npy  \n",
      "  inflating: data/word_to_index.json  \n",
      "  inflating: __MACOSX/data/._word_to_index.json  \n",
      "  inflating: data/docs_train.npy     \n",
      "  inflating: __MACOSX/data/._docs_train.npy  \n"
     ]
    }
   ],
   "source": [
    "# In case you are using google colab:\n",
    "# uncomment the following two lines: \n",
    "\n",
    "#%tensorflow_version 1.9\n",
    "#!pip install keras==2.2.5\n",
    "\n",
    "!wget -c \"https://onedrive.live.com/download?cid=AE69638675180117&resid=AE69638675180117%2199289&authkey=AHgxt3xmgG0Fu5A\" -O \"data.zip\"\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJaSJaIP1xRy"
   },
   "source": [
    "# = = = = = Attention Layer = = = = ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MeXk--0rncj4"
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    https://github.com/richliao/textClassifier/issues/13#issuecomment-377323318\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yoM7H0KQncpF"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Layer as Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    \n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    \n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, return_coefficients=False,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.return_coefficients = return_coefficients\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        \n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        \n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        \n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "        \n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "        \n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "        \n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "        \n",
    "        a = K.exp(ait)\n",
    "        \n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        \n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        \n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x*a ### fill the gap ### # compute the attentional vector\n",
    "        \n",
    "        if self.return_coefficients:\n",
    "            return  [K.sum(weighted_input, axis=1), a] ### fill the gap - [attentional vector, coefficients] ###\n",
    "        else:\n",
    "            return K.sum(weighted_input, axis=1)### fill the gap - attentional vector only ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_coefficients:\n",
    "            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB-WOy4YHlsr"
   },
   "source": [
    "# = = = = = Bidirectional GRU = = = = =\n",
    "#### fill the gaps in the bidir_gru function below ###\n",
    "#### add a RNN-GRU layer and a bidirectional wrapper ###\n",
    "#### bidirectional: search for 'bidirectional' [here](https://keras.io/layers/wrappers/)\n",
    "#### GRU: search for 'GRU' [here](https://keras.io/layers/recurrent/)\n",
    "#### layers can be combined by nesting them as: layer_b(parameters_b)(layer_a(parameters_a)(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qoFkuGwNncwy"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, GRU\n",
    "\n",
    "def bidir_gru(my_seq,n_units):\n",
    "    '''\n",
    "    just a convenient wrapper for bidirectional RNN with GRU units\n",
    "    '''\n",
    "    return Bidirectional(GRU(units=n_units, activation='tanh',return_sequences=True),merge_mode='concat')(my_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgTP6GrOHlss"
   },
   "source": [
    "# = = = = = Parameters = = = = ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czsVjxgYnczb"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "path_root = ''\n",
    "path_to_data = path_root + 'data/'\n",
    "\n",
    "d = 30 # dimensionality of word embeddings\n",
    "n_units = 50 # RNN layer dimensionality\n",
    "drop_rate = 0.5 # dropout\n",
    "mfw_idx = 2 # index of the most frequent words in the dictionary \n",
    "            # 0 is for the special padding token\n",
    "            # 1 is for the special out-of-vocabulary token\n",
    "\n",
    "padding_idx = 0\n",
    "oov_idx = 1\n",
    "batch_size = 32\n",
    "nb_epochs = 6\n",
    "my_optimizer = 'adam'\n",
    "my_patience = 2 # for early stopping strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8Vot_C7Hlst"
   },
   "source": [
    "# = = = = = Data Loading = = = = ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UD6hRh0OHlst"
   },
   "outputs": [],
   "source": [
    "my_docs_array_train = np.load(path_to_data + 'docs_train.npy')\n",
    "my_docs_array_test = np.load(path_to_data + 'docs_test.npy')\n",
    "\n",
    "my_labels_array_train = np.load(path_to_data + 'labels_train.npy')\n",
    "my_labels_array_test = np.load(path_to_data + 'labels_test.npy')\n",
    "\n",
    "# load dictionary of word indexes (sorted by decreasing frequency across the corpus)\n",
    "with open(path_to_data + 'word_to_index.json', 'r') as my_file:\n",
    "    word_to_index = json.load(my_file)\n",
    "\n",
    "# invert mapping\n",
    "index_to_word = dict((v,k) for k,v in word_to_index.items()) ### fill the gap (use a dict comprehension) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rzqEGOdHlst"
   },
   "source": [
    "# = = = = = Defining Architecture = = = = ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMj9j1_pHlst",
    "outputId": "29962c53-9cd7-42bd-af7c-03aed73bab22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model compiled\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dropout, TimeDistributed, Dense\n",
    "\n",
    "sent_ints = Input(shape=(my_docs_array_train.shape[2],)) # vec of ints of variable size\n",
    "\n",
    "sent_wv = Embedding(input_dim=len(index_to_word)+2, # vocab size\n",
    "                    output_dim=d, # dimensionality of embedding space\n",
    "                    input_length=my_docs_array_train.shape[2],\n",
    "                    trainable=True\n",
    "                    )(sent_ints)\n",
    "\n",
    "sent_wv_dr = Dropout(drop_rate)(sent_wv)\n",
    "sent_wa = bidir_gru(sent_wv_dr,n_units)### fill the gap ### # get the annotations for each word in the sent\n",
    "sent_att_vec,word_att_coeffs = AttentionWithContext(return_coefficients=True)(sent_wa) ### fill the gap ### # get the attentional vector for the sentence\n",
    "sent_att_vec_dr = Dropout(drop_rate)(sent_att_vec)                      \n",
    "sent_encoder = Model(sent_ints,sent_att_vec_dr)\n",
    "\n",
    "doc_ints = Input(shape=(my_docs_array_train.shape[1],my_docs_array_train.shape[2],))\n",
    "sent_att_vecs_dr = TimeDistributed(sent_encoder)(doc_ints) ### fill the gap ### # apply the sentence encoder model to each sentence in the document. Search for 'TimeDistributed' in https://keras.io/layers/wrappers/\n",
    "doc_sa = bidir_gru(sent_att_vecs_dr,n_units) ### fill the gap ### # get annotations for each sent in the doc\n",
    "doc_att_vec,sent_att_coeffs = AttentionWithContext(return_coefficients=True)(doc_sa) ### fill the gap ### # get attentional vector for the doc\n",
    "doc_att_vec_dr = Dropout(drop_rate)(doc_att_vec)\n",
    "                  \n",
    "preds = Dense(units=1,\n",
    "              activation='sigmoid')(doc_att_vec_dr)\n",
    "model = Model(doc_ints,preds)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer = my_optimizer,\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print('model compiled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgreR5AcHlst"
   },
   "source": [
    "# = = = = = Training = = = = ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rbW_vheGHlst",
    "outputId": "f46cfa79-4f28-4658-c831-bf216212dc06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "782/782 [==============================] - 45s 45ms/step - loss: 0.4654 - accuracy: 0.7565 - val_loss: 0.3607 - val_accuracy: 0.8423\n",
      "Epoch 2/6\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.2583 - accuracy: 0.8970 - val_loss: 0.3591 - val_accuracy: 0.8451\n",
      "Epoch 3/6\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.1766 - accuracy: 0.9326 - val_loss: 0.4275 - val_accuracy: 0.8300\n",
      "Epoch 4/6\n",
      "782/782 [==============================] - 33s 42ms/step - loss: 0.1214 - accuracy: 0.9549 - val_loss: 0.5061 - val_accuracy: 0.8275\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "loading_pretrained = False\n",
    "\n",
    "if not loading_pretrained:\n",
    "    early_stopping = EarlyStopping(monitor='val_acc',\n",
    "                                   patience=my_patience,\n",
    "                                   mode='max')\n",
    "    \n",
    "    # save model corresponding to best epoch\n",
    "    checkpointer = ModelCheckpoint(filepath=path_to_data + 'model', \n",
    "                                   verbose=1, \n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True)\n",
    "    \n",
    "    # 200s/epoch on CPU - reaches 84.38% accuracy in 2 epochs\n",
    "    ### fill the gap ### # call the .fit() method on your model with the arguments: my_docs_array_train, my_labels_array_train, batch_size, nb_epochs, my_docs_array_test, my_labels_array_test, early_stopping\n",
    "    # look at: https://keras.io/models/sequential/#fit\n",
    "    model.fit(my_docs_array_train, my_labels_array_train, batch_size, nb_epochs,validation_data = (my_docs_array_test, my_labels_array_test),callbacks= [early_stopping]) # checkpointer\n",
    "\n",
    "else:\n",
    "    model.load_weights(path_to_data + 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvyr8B5QHlst"
   },
   "source": [
    "# = = = = = Extraction of Attention Coefficients = = = = ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVr8cS4MHlst"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "# define intermediate models\n",
    "### fill the two gaps below ###\n",
    "get_word_att_coeffs = Model(sent_ints, word_att_coeffs) # attention coeffs over the words in a sent\n",
    "get_sent_attention_coeffs = Model(doc_ints, sent_att_coeffs) # attention coeffs over the sents in the doc\n",
    "\n",
    "my_review = my_docs_array_test[-1:,:,:] # select last review\n",
    "# convert integer review to text\n",
    "index_to_word[1] = 'OOV'\n",
    "my_review_text = [[index_to_word[idx] for idx in sent if idx in index_to_word] for sent in my_review.tolist()[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHDJ7JiqHlsu"
   },
   "source": [
    "# = = = = = Attention Over Sentences in the Document = = = = ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yooWg3kkHlsu",
    "outputId": "4898ed42-ed6b-4221-91d6-34a6cdec98b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.06 There 's a sign on The Lost Highway that says : OOV SPOILERS OOV ( but you already knew that , did n't you ? )\n",
      "8.22 Since there 's a great deal of people that apparently did not get the point of this movie , I 'd like to contribute my interpretation of why the plot\n",
      "10.83 As others have pointed out , one single viewing of this movie is not sufficient .\n",
      "17.47 If you have the DVD of MD , you can OOV ' by looking at David Lynch 's 'Top 10 OOV to OOV MD ' ( but only upon second\n",
      "25.12 ; ) First of all , Mulholland Drive is downright brilliant .\n",
      "20.81 A masterpiece .\n",
      "12.49 This is the kind of movie that refuse to leave your head .\n"
     ]
    }
   ],
   "source": [
    "sent_coeffs = get_sent_attention_coeffs.predict(my_review)\n",
    "sent_coeffs = sent_coeffs[0,:,:]\n",
    "\n",
    "for elt in zip(sent_coeffs[:,0].tolist(),[' '.join(elt) for elt in my_review_text]):\n",
    "    print(round(elt[0]*100,2),elt[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rII-DNrKHlsu"
   },
   "source": [
    "# = = = = = Attention Over Words in Each Sentence = = = = ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyFjAga6Hlsu",
    "outputId": "7caece77-3a68-4229-c6c0-9cf9a2a4b77e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('There', 0.18)\n",
      "(\"'s\", 0.23)\n",
      "('a', 0.29)\n",
      "('sign', 0.19)\n",
      "('on', 0.38)\n",
      "('The', 0.26)\n",
      "('Lost', 0.82)\n",
      "('Highway', 0.25)\n",
      "('that', 0.23)\n",
      "('says', 0.16)\n",
      "(':', 0.19)\n",
      "('OOV', 0.16)\n",
      "('SPOILERS', 0.3)\n",
      "('OOV', 0.21)\n",
      "('(', 0.14)\n",
      "('but', 0.13)\n",
      "('you', 0.08)\n",
      "('already', 0.06)\n",
      "('knew', 0.1)\n",
      "('that', 0.09)\n",
      "(',', 0.1)\n",
      "('did', 0.08)\n",
      "(\"n't\", 0.05)\n",
      "('you', 0.06)\n",
      "('?', 0.09)\n",
      "(')', 0.15)\n",
      "= = = =\n",
      "('Since', 1.29)\n",
      "('there', 0.69)\n",
      "(\"'s\", 0.59)\n",
      "('a', 0.61)\n",
      "('great', 0.54)\n",
      "('deal', 0.12)\n",
      "('of', 0.14)\n",
      "('people', 0.03)\n",
      "('that', 0.03)\n",
      "('apparently', 0.02)\n",
      "('did', 0.02)\n",
      "('not', 0.01)\n",
      "('get', 0.03)\n",
      "('the', 0.03)\n",
      "('point', 0.07)\n",
      "('of', 0.1)\n",
      "('this', 0.17)\n",
      "('movie', 0.17)\n",
      "(',', 0.24)\n",
      "('I', 0.17)\n",
      "(\"'d\", 0.27)\n",
      "('like', 0.17)\n",
      "('to', 0.29)\n",
      "('contribute', 0.75)\n",
      "('my', 0.2)\n",
      "('interpretation', 0.25)\n",
      "('of', 0.17)\n",
      "('why', 0.15)\n",
      "('the', 0.31)\n",
      "('plot', 0.59)\n",
      "= = = =\n",
      "('As', 0.15)\n",
      "('others', 0.18)\n",
      "('have', 0.11)\n",
      "('pointed', 0.09)\n",
      "('out', 0.17)\n",
      "(',', 0.18)\n",
      "('one', 0.39)\n",
      "('single', 0.75)\n",
      "('viewing', 2.1)\n",
      "('of', 0.94)\n",
      "('this', 1.06)\n",
      "('movie', 0.72)\n",
      "('is', 1.0)\n",
      "('not', 0.22)\n",
      "('sufficient', 1.66)\n",
      "('.', 0.96)\n",
      "= = = =\n",
      "('If', 0.52)\n",
      "('you', 0.26)\n",
      "('have', 0.29)\n",
      "('the', 0.46)\n",
      "('DVD', 1.2)\n",
      "('of', 0.89)\n",
      "('MD', 0.54)\n",
      "(',', 0.5)\n",
      "('you', 0.19)\n",
      "('can', 0.36)\n",
      "('OOV', 0.35)\n",
      "(\"'\", 0.66)\n",
      "('by', 0.43)\n",
      "('looking', 0.35)\n",
      "('at', 0.46)\n",
      "('David', 0.87)\n",
      "('Lynch', 0.48)\n",
      "(\"'s\", 0.67)\n",
      "(\"'Top\", 1.92)\n",
      "('10', 1.44)\n",
      "('OOV', 1.03)\n",
      "('to', 0.85)\n",
      "('OOV', 0.67)\n",
      "('MD', 0.46)\n",
      "(\"'\", 0.59)\n",
      "('(', 0.17)\n",
      "('but', 0.16)\n",
      "('only', 0.12)\n",
      "('upon', 0.37)\n",
      "('second', 0.2)\n",
      "= = = =\n",
      "(';', 1.94)\n",
      "(')', 2.89)\n",
      "('First', 2.05)\n",
      "('of', 2.26)\n",
      "('all', 1.24)\n",
      "(',', 1.54)\n",
      "('Mulholland', 1.41)\n",
      "('Drive', 0.82)\n",
      "('is', 1.28)\n",
      "('downright', 3.78)\n",
      "('brilliant', 3.13)\n",
      "('.', 2.52)\n",
      "= = = =\n",
      "('A', 6.29)\n",
      "('masterpiece', 8.06)\n",
      "('.', 5.67)\n",
      "= = = =\n",
      "('This', 0.91)\n",
      "('is', 1.59)\n",
      "('the', 1.39)\n",
      "('kind', 0.9)\n",
      "('of', 1.45)\n",
      "('movie', 1.1)\n",
      "('that', 0.79)\n",
      "('refuse', 1.8)\n",
      "('to', 0.51)\n",
      "('leave', 0.17)\n",
      "('your', 0.54)\n",
      "('head', 0.57)\n",
      "('.', 0.62)\n",
      "= = = =\n",
      "('Lost', 0.82)\n",
      "('on', 0.38)\n",
      "('SPOILERS', 0.3)\n",
      "('a', 0.29)\n",
      "('The', 0.26)\n",
      "('Highway', 0.25)\n",
      "(\"'s\", 0.23)\n",
      "('that', 0.23)\n",
      "('OOV', 0.21)\n",
      "('sign', 0.19)\n",
      "(':', 0.19)\n",
      "('There', 0.18)\n",
      "('says', 0.16)\n",
      "('OOV', 0.16)\n",
      "(')', 0.15)\n",
      "('(', 0.14)\n",
      "('but', 0.13)\n",
      "('knew', 0.1)\n",
      "(',', 0.1)\n",
      "('that', 0.09)\n",
      "('?', 0.09)\n",
      "('you', 0.08)\n",
      "('did', 0.08)\n",
      "('already', 0.06)\n",
      "('you', 0.06)\n",
      "(\"n't\", 0.05)\n",
      "= = = =\n",
      "('Since', 1.29)\n",
      "('contribute', 0.75)\n",
      "('there', 0.69)\n",
      "('a', 0.61)\n",
      "(\"'s\", 0.59)\n",
      "('plot', 0.59)\n",
      "('great', 0.54)\n",
      "('the', 0.31)\n",
      "('to', 0.29)\n",
      "(\"'d\", 0.27)\n",
      "('interpretation', 0.25)\n",
      "(',', 0.24)\n",
      "('my', 0.2)\n",
      "('this', 0.17)\n",
      "('movie', 0.17)\n",
      "('I', 0.17)\n",
      "('like', 0.17)\n",
      "('of', 0.17)\n",
      "('why', 0.15)\n",
      "('of', 0.14)\n",
      "('deal', 0.12)\n",
      "('of', 0.1)\n",
      "('point', 0.07)\n",
      "('people', 0.03)\n",
      "('that', 0.03)\n",
      "('get', 0.03)\n",
      "('the', 0.03)\n",
      "('apparently', 0.02)\n",
      "('did', 0.02)\n",
      "('not', 0.01)\n",
      "= = = =\n",
      "('viewing', 2.1)\n",
      "('sufficient', 1.66)\n",
      "('this', 1.06)\n",
      "('is', 1.0)\n",
      "('.', 0.96)\n",
      "('of', 0.94)\n",
      "('single', 0.75)\n",
      "('movie', 0.72)\n",
      "('one', 0.39)\n",
      "('not', 0.22)\n",
      "('others', 0.18)\n",
      "(',', 0.18)\n",
      "('out', 0.17)\n",
      "('As', 0.15)\n",
      "('have', 0.11)\n",
      "('pointed', 0.09)\n",
      "= = = =\n",
      "(\"'Top\", 1.92)\n",
      "('10', 1.44)\n",
      "('DVD', 1.2)\n",
      "('OOV', 1.03)\n",
      "('of', 0.89)\n",
      "('David', 0.87)\n",
      "('to', 0.85)\n",
      "(\"'s\", 0.67)\n",
      "('OOV', 0.67)\n",
      "(\"'\", 0.66)\n",
      "(\"'\", 0.59)\n",
      "('MD', 0.54)\n",
      "('If', 0.52)\n",
      "(',', 0.5)\n",
      "('Lynch', 0.48)\n",
      "('the', 0.46)\n",
      "('at', 0.46)\n",
      "('MD', 0.46)\n",
      "('by', 0.43)\n",
      "('upon', 0.37)\n",
      "('can', 0.36)\n",
      "('OOV', 0.35)\n",
      "('looking', 0.35)\n",
      "('have', 0.29)\n",
      "('you', 0.26)\n",
      "('second', 0.2)\n",
      "('you', 0.19)\n",
      "('(', 0.17)\n",
      "('but', 0.16)\n",
      "('only', 0.12)\n",
      "= = = =\n",
      "('downright', 3.78)\n",
      "('brilliant', 3.13)\n",
      "(')', 2.89)\n",
      "('.', 2.52)\n",
      "('of', 2.26)\n",
      "('First', 2.05)\n",
      "(';', 1.94)\n",
      "(',', 1.54)\n",
      "('Mulholland', 1.41)\n",
      "('is', 1.28)\n",
      "('all', 1.24)\n",
      "('Drive', 0.82)\n",
      "= = = =\n",
      "('masterpiece', 8.06)\n",
      "('A', 6.29)\n",
      "('.', 5.67)\n",
      "= = = =\n",
      "('refuse', 1.8)\n",
      "('is', 1.59)\n",
      "('of', 1.45)\n",
      "('the', 1.39)\n",
      "('movie', 1.1)\n",
      "('This', 0.91)\n",
      "('kind', 0.9)\n",
      "('that', 0.79)\n",
      "('.', 0.62)\n",
      "('head', 0.57)\n",
      "('your', 0.54)\n",
      "('to', 0.51)\n",
      "('leave', 0.17)\n",
      "= = = =\n"
     ]
    }
   ],
   "source": [
    "from keras.backend import _to_tensor\n",
    "\n",
    "my_review_tensor = _to_tensor(my_review,dtype='float32') # a layer, unlike a model, requires a TensorFlow tensor as input\n",
    "\n",
    "word_coeffs = get_word_att_coeffs(my_review_tensor[0,:,:]) ### fill the gap ### # get the word attentional coefficients for each sentence in the document\n",
    "word_coeffs = K.eval(word_coeffs) # shape = (1, 7, 30, 1): (batch size, nb of sents in doc, nb of words per sent, coeff)\n",
    "word_coeffs = word_coeffs[:,:,0] # shape = (7, 30) (coeff for each word in each sentence)\n",
    "word_coeffs = sent_coeffs * word_coeffs # re-weight by sentence importance\n",
    "word_coeffs = np.round((word_coeffs*100).astype(np.float64),2)\n",
    "\n",
    "word_coeffs_list = word_coeffs.tolist()\n",
    "\n",
    "# match text and coefficients\n",
    "text_word_coeffs = [list(zip(words,word_coeffs_list[idx][:len(words)])) for idx,words in enumerate(my_review_text)]\n",
    "\n",
    "for sent in text_word_coeffs:\n",
    "    [print(elt) for elt in sent]  \n",
    "    print('= = = =')\n",
    "\n",
    "# sort words by importance within each sentence\n",
    "text_word_coeffs_sorted = [sorted(elt,key=operator.itemgetter(1),reverse=True) for elt in text_word_coeffs]\n",
    "\n",
    "for sent in text_word_coeffs_sorted:\n",
    "    [print(elt) for elt in sent]\n",
    "    print('= = = =')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvfRDoAgq2Py"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " ALTEGRAD_2021_Lab1_HAN_Handout.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
